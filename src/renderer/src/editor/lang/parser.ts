// This file was generated by lezer-generator. You probably shouldn't edit it.
import { LRParser } from "@lezer/lr";
import { noteContent } from "./external-tokens";

export const parser = LRParser.deserialize({
  version: 14,
  states:
    "!pQQOPOOOVOPO'#C`O[OQO'#C_OOOO'#Cd'#CdQQOPOOOaOPO,58zOOOO'#Cc'#CcOOOO,58y,58yOOOO-E6b-E6bOfOPO1G.fOOOQ7+$Q7+$QOnOPO7+$QOOOQ<<Gl<<Gl",
  stateData: "s~OYPO~OZTO~OPUO~OTXO~OUZOYYO~OY[O~O",
  goto: "kXPPPY^PPbeTROSTQOSRVQQSORWS",
  nodeNames: "âš  NoteInnerContent Document Note NoteDelimiter NoteLanguage Auto NoteContent",
  maxTerm: 11,
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData:
    "(f~R[YZw}!O|#V#W!X#[#]!j#^#_!|#`#a$h#a#b%Q#d#e&P#g#h&r#h#i'X#m#n'}%&x%&y(T~|OY~~!PP#T#U!S~!XOU~~![P#g#h!_~!bP#g#h!e~!jOT~~!mP#h#i!p~!sP#a#b!v~!yP#`#a!e~#PQ#T#U#V#g#h$X~#YP#j#k#]~#`P#T#U#c~#fP#g#h#i~#lP#V#W#o~#rP#f#g#u~#xP#]#^#{~$OP#d#e$R~$UP#h#i!e~$[Q#c#d$b#l#m!e~$eP#b#c!e~$kP#X#Y$n~$qP#n#o$t~$wP#X#Y$z~$}P#f#g!e~%TP#T#U%W~%ZQ#f#g%a#h#i%y~%dP#_#`%g~%jP#W#X%m~%pP#c#d%s~%vP#k#l$b~%|P#[#]!e~&SQ#[#]&Y#m#n&`~&]P#d#e!e~&cP#h#i&f~&iP#[#]&l~&oP#c#d$b~&uQ#[#]&{#e#f!v~'OP#X#Y'R~'UP#`#a!v~'[R#X#Y'e#g#h'k#m#n'q~'hP#l#m$R~'nP#l#m!e~'tP#d#e'w~'zP#X#Y#c~(QP#T#U!p~(WP%&x%&y(Z~(^P%&x%&y(a~(fOZ~",
  tokenizers: [0, noteContent],
  topRules: { Document: [0, 2] },
  tokenPrec: 0
});
